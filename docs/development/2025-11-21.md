# 2025-11-21 개발 로그

- **상태 요약**: LangGraph 노드부터 FastAPI 스트림까지 전체 호출 체인을 비동기/Send API 기반 병렬 실행으로 전환
- **변경 사항**:
  1. 모든 LLM 노드를 `async def`로 변경하고 공용 `_ainvoke` 헬퍼로 `ainvoke` 우선 호출, 미지원 모델은 스레드풀에서 실행
  2. Send API 기반 `dispatch_llm_calls`를 통해 `init_question`에서 5개 LLM 노드를 동시에 fan-out
  3. `stream_graph`를 `app.astream` 기반 비동기 제너레이터로 수정해 LangGraph 이벤트를 asyncio 스트림으로 노출
  4. `/api/ask` 라우터의 스트리밍 응답을 비동기 제너레이터로 재작성해 FastAPI가 이벤트를 즉시 push하고, 완료 순서/최초 응답/오류 정보를 요약에 포함
  5. Streamlit 클라이언트를 `httpx.stream()` 기반으로 교체하고 UI에 최초 완료 모델과 오류 로그를 강조
  6. `app/logger.py` 이모지 로거를 추가하고 서비스/라우터/Streamlit/실행 스크립트 전반에 적용해 실행 상황을 콘솔에서 모니터링 가능
  7. Streamlit 대시보드를 `st.chat_input` 기반 챗봇 UI로 개편해 히스토리를 위/입력창을 아래에 배치하고, 메시지 로그 섹션을 제거
- **다음 단계**:
  1. Streamlit 클라이언트에서 asyncio/HTTPX 전환 여부 평가
  2. Send API fan-out 이후 결과를 집계/우선순위화할 후처리 전략 검토
  3. 오류 이벤트를 기반으로 자동 재시도/알림 훅을 붙이는 방안 모색
